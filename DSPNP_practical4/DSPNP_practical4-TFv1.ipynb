{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Session 4: Getting Started with Deep Learning Models in TensorFlow\n",
    "\n",
    "*Notebook by [Marek Rei](https://github.com/marekrei/cl-datasci-pnp)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical, we will continue from where the lecture left off and learn more about using TensorFlow.\n",
    "\n",
    "The practical will cover a few different network architectures and we will look at different components that are often used in neural networks.\n",
    "\n",
    "To start off, let's import `TensorFlow` into our notebook. If you have just installed `TensorFlow` you will have version 2.X, which is a more user-friendly version of this library and which packs many details in concise function calls. In costrast, `TensorFlow` 1.X is a more verbose and a lower-level version; it is still useful to \"look under the hood\", and you can in fact run the code from `TensorFlow` 1.X using `TensorFlow` 2.X in a compatibility mode like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using TensorFlow 1.X\n",
    "#import tensorflow as tf\n",
    "\n",
    "# Otherwise, you can still run code from this notebook \n",
    "# with TensorFlow 2.X using its compatibility mode\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Tensorflow Example\n",
    "\n",
    "This is the first example from the lecture. \n",
    "\n",
    "We first create a network with two placeholders that adds these together and returns the result. Then, we execute this network with two input values, 4 and 5. This returns the result 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.placeholder(tf.float32, name=\"a\")\n",
    "b = tf.placeholder(tf.float32, name=\"b\")\n",
    "y = a + b\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(y,\n",
    "                      feed_dict={a:4, b:5})\n",
    "    print(\"Result: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occasionally throughout this notebook, the following function will be called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is necessary to reset the TensorFlow network. We have many different small networks in one notebook and we don't want them interfering with each other, so as a pre-emptive measure we will occasionally reset the computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Parameters\n",
    "\n",
    "This is the second example from the lecture, showing how to optimize the parameters in your model.\n",
    "\n",
    "We define a network that takes a vector `x` with two features as input, multiplies the features with corresponding parameters in `W`, and sums them together. We then train this network for 10 epochs over a single training point, optimizing the output towards value 20. Printing out the results, we can see that the output `y` gradually moves towards the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [2], name=\"x\")\n",
    "target = tf.placeholder(tf.float32, name=\"target\")\n",
    "learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "\n",
    "W = tf.get_variable(\"W\", initializer=[0.2, 0.7])\n",
    "y = tf.reduce_sum(x * W)\n",
    "\n",
    "loss = tf.pow(target - y, 2.0)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(10):\n",
    "        result, _ = sess.run(\n",
    "            [y, train_op], \n",
    "            feed_dict={x: [1.0, 1.0], \n",
    "                       target: 20.0, \n",
    "                       learning_rate: 0.1}) \n",
    "        print(\"Result: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Layers\n",
    "\n",
    "For most cases, we don't actually need to create the trainable variables manually. Instead, the feedfoward layer is available as a pre-defined module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 2], name=\"x\")\n",
    "y = tf.layers.dense(x, 1, activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a hidden layer that takes `x` as input, has 1 output neuron (we can also create bigger layers of course), and has no non-linear activation. The parameters that connect the two layers together are created automatically and are trained during optimization. By default, these parameters are initialized randomly.\n",
    "\n",
    "Let's replace the manually created variables with a TensorFlow dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2], name=\"x\")\n",
    "target = tf.placeholder(tf.float32, name=\"target\")\n",
    "learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "\n",
    "y = tf.layers.dense(x, 1, activation=None)\n",
    "\n",
    "loss = tf.pow(target - y, 2.0)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(10):\n",
    "        result, _ = sess.run(\n",
    "            [y, train_op], \n",
    "            feed_dict={x: [[1.0, 1.0]], \n",
    "                       target: 20.0, \n",
    "                       learning_rate: 0.1}) \n",
    "        print(\"Result: \", result[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version actually gets to the correct solution a bit faster than before. That's because it is internally also creating a bias parameter, which adds a bit more power to the model.\n",
    "\n",
    "In large networks, you would normally chain together many large layers with non-linear activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 300], name=\"x\")\n",
    "hidden1 = tf.layers.dense(x, 100, activation=tf.tanh)\n",
    "hidden2 = tf.layers.dense(hidden1, 50, activation=tf.tanh)\n",
    "y = tf.layers.dense(hidden2, 1, activation=tf.sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "In the last example, we used non-linear activation functions. As we saw in the lectures, this is what gives neural networks their power to model non-linear patterns in the data. There are a number of different activation functions to choose from.\n",
    "\n",
    "The [sigmoid function](https://en.wikipedia.org/wiki/Logistic_function), also known as the logistic function, is the most classic non-linear activation. It transforms the value to a range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = tf.layers.dense(x, 100, activation=tf.sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In modern networks, the [tanh function](https://en.wikipedia.org/wiki/Hyperbolic_function) is used more often. It has more flexibility, as it transforms the input value to a range between -1 and 1, and can therefore output negative values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = tf.layers.dense(x, 100, activation=tf.tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular one is the [Rectified Linear Unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) function, or the ReLU. This function acts as a linear function above zero, but restricts everything below zero to 0. By doing this it also introduces a non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = tf.layers.dense(x, 100, activation=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial linear property of the ReLU can help it converge faster on some tasks, although in practice tanh may be a more robust option.\n",
    "\n",
    "Finally, [softmax](https://en.wikipedia.org/wiki/Softmax_function) is a special type of activation function. It takes a whole layer as input and converts it into a probability distribution, such that all values are between 0 and 1, and together they sum up to 1. It is often used in the output layers of networks when performing classification, in order to predict a probability distribution over all the possible classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.layers.dense(hidden, 2, activation=None)\n",
    "probabilities = tf.nn.softmax(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations and Useful Functions\n",
    "\n",
    "TensorFlow has corresponding versions of all the main operations you might want to use. This means you can add them into your computation graph and into your neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.abs # absolute value\n",
    "tf.negative # computes the negative value\n",
    "tf.sign # returns 1, 0 or -1 depending on the sign of the input\n",
    "tf.reciprocal # reciprocal 1/x\n",
    "tf.square # return input squared\n",
    "tf.round # return rounded value\n",
    "tf.sqrt # square root\n",
    "tf.rsqrt # reciprocal of square root\n",
    "tf.pow # power\n",
    "tf.exp # exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These operations can be applied to scalar values, but also to vectors, matrices and higher-order tensors. In the latter case, they will be applied element-wise. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.placeholder(tf.int32, name=\"a\")\n",
    "b = tf.placeholder(tf.float32, [3], name=\"b\")\n",
    "\n",
    "c = tf.negative(a)\n",
    "d = tf.square(b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    c_, d_ = sess.run([c, d], feed_dict={a:4, b:[3.0,2.0,1.0]})\n",
    "    print(c_, d_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful operations are performed over a whole vector/matrix tensor and return a single value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum # Add elements together\n",
    "tf.reduce_mean # Average over elements\n",
    "tf.reduce_min # Minimum value\n",
    "tf.reduce_max # Maximum value\n",
    "tf.argmax # Index of the largest value\n",
    "tf.argmin # Index of the smallest value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "b = tf.placeholder(tf.float32, [3,2], name=\"b\")\n",
    "c = tf.reduce_sum(b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    c_ = sess.run([c], feed_dict={b:[[6.0, 5.0],[4.0,3.0],[2.0,1.0]]})\n",
    "    print(c_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different adaptive learning rate strategies are also implemented in TensorFlow as functions. The main ones to try are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.GradientDescentOptimizer\n",
    "tf.train.AdadeltaOptimizer\n",
    "tf.train.AdamOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in the differences between these strategies, [this blog post](http://ruder.io/optimizing-gradient-descent/) provides more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an XOR Function\n",
    "\n",
    "[XOR](https://en.wikipedia.org/wiki/XOR_gate) is the function that takes two binary values and returns 1 only if one of them is 1 and the other 0, while returning 0 if both of them have the same value.\n",
    "\n",
    "It can be a complicated function to optimize and cannot be modeled with a linear model. But let's try anyway.\n",
    "\n",
    "Our dataset consists of all the possible different states that XOR can take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\n",
    "data_y = [0.0, 1.0, 1.0, 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct a linear network and optimize it on this dataset, printing out the predictions at each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2], name=\"x\")\n",
    "target = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "\n",
    "y = tf.reduce_sum(tf.layers.dense(x, 1, activation=None), axis=1)\n",
    "\n",
    "loss = tf.reduce_sum(tf.pow(target - y, 2.0))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "data_x = [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\n",
    "data_y = [0.0, 1.0, 1.0, 0.0]\n",
    "lr = 0.1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(50):\n",
    "        result, _ = sess.run([y, train_op], feed_dict={x: data_x, target: data_y, learning_rate: lr})\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch \", epoch, \": \", \"\\t\".join([str(x) for x in result]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it's not doing very well. Ideally, the predictions should be [0, 1, 1, 0], but in this case they are hovering around 0.5 for every input case.\n",
    "\n",
    "In order to improve this architecture, let's add some non-linear layers into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2], name=\"x\")\n",
    "target = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "\n",
    "hidden = tf.layers.dense(x, 5, activation=tf.tanh) # <- non-linear layer\n",
    "y = tf.reduce_sum(tf.layers.dense(hidden, 1, activation=tf.sigmoid), axis=1) # <- non-linear layer\n",
    "\n",
    "loss = tf.reduce_sum(tf.pow(target - y, 2.0))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "data_x = [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\n",
    "data_y = [0.0, 1.0, 1.0, 0.0]\n",
    "lr = 1.0\n",
    "\n",
    "tf.set_random_seed(20)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(50):\n",
    "        result, _ = sess.run([y, train_op], feed_dict={x: data_x, target: data_y, learning_rate: lr})\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch \", epoch, \": \", \"\\t\".join([str(x) for x in result]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better. The values are much closer to [0, 1, 1, 0] than before, and they will continue improving if we train for longer.\n",
    "\n",
    "We also had to increase the learning rate for this network. It was still learning with the smaller learning rate, but was convering very slowly. As we discussed in the lectures, learning rate is a hyperparameter that can vary quite a bit depending on the network architecture and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR Classification\n",
    "\n",
    "We can also do classification with TensorFlow. For this, we often use the softmax activation function described above, which predicts the probability for each of the possible classes.\n",
    "\n",
    "We also have to change the loss function, as squared error is not suitable for classification. The loss function that works best with softmax is [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy). When minimizing cross entropy, we are essentially minimizing the negative log likelihood of the correct class for each datapoint. That's exactly what we want, as the model learns to assign high values for the correct label.\n",
    "\n",
    "We can change the XOR example above to perform classification instead. In this case, we are constructing a binary classifier - choosing between the classes of 0 and 1. When printing the output, we are printing the predicted classes, which were assigned the highest probability by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2], name=\"x\")\n",
    "target = tf.placeholder(tf.int32, [None], name=\"target\")\n",
    "learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "\n",
    "hidden = tf.layers.dense(x, 5, activation=tf.tanh)\n",
    "output = tf.layers.dense(hidden, 2, activation=None)\n",
    "\n",
    "probabilities = tf.nn.softmax(output)\n",
    "predictions = tf.argmax(probabilities, axis=1)\n",
    "loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=target)\n",
    "loss = tf.reduce_mean(loss_)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "data_x = [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\n",
    "data_targets = [0, 1, 1, 0]\n",
    "lr = 1.0\n",
    "\n",
    "tf.set_random_seed(20)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(50):\n",
    "        result, _ = sess.run([predictions, train_op], feed_dict={x: data_x, target: data_targets, learning_rate: lr})\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch \", epoch, \": \", \" \".join([str(x) for x in result]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model starts off with incorrect predictions, but fairly soon learns to return the correct sequence of [0, 1, 1, 0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
